{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import getClasses\n",
    "from functions import createFolder\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('/home/arclab/Documents/FlorianHwk/ECE271B/')\n",
    "from pyAudioAnalysis import audioFeatureExtraction as aT\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate folders of features!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No need to run if you have the train_Features and test_Features Folders. Put them inside the /data/folder\n",
    "\n",
    "ourClasses, _, _, _ = getClasses('classes.csv', \"data/ontology.json\", 'data/class_labels_indices.csv')\n",
    "\n",
    "createFolder(\"data/train_Features\")\n",
    "    \n",
    "for c in ourClasses:\n",
    "    directory = \"data/train_rawAudio/\" + c + \"/\"\n",
    "    features, names = aT.dirWavFeatureExtraction(directory, 1.0, 1.0, 0.05, 0.05)\n",
    "    with open('data/train_Features/' + c + '.csv', 'wb') as csvfile:\n",
    "        csvWriter = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        for i in range(0, min(len(names),len(features))):            \n",
    "            row = [\"%.10f\" % feat for feat in features[i]]\n",
    "            row.insert(0,names[i])\n",
    "            csvWriter.writerow(row)\n",
    "            \n",
    "createFolder(\"data/test_Features\")\n",
    "    \n",
    "for c in ourClasses:\n",
    "    directory = \"data/test_rawAudio/\" + c + \"/\"\n",
    "    features, names = aT.dirWavFeatureExtraction(directory, 1.0, 1.0, 0.05, 0.05)\n",
    "    with open('data/test_Features/' + c + '.csv', 'wb') as csvfile:\n",
    "        csvWriter = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        for i in range(0, len(names)):            \n",
    "            row = [\"%.20f\" % feat for feat in features[i]]\n",
    "            row.insert(0,names[i])\n",
    "            csvWriter.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assumes featureFolder is generated. Look at generateFeatures.ipynb\n",
    "def getFeatureMatrices(pathToFeatureCSV):\n",
    "    data = [];\n",
    "    numComma = pathToFeatureCSV.count(',')\n",
    "    with open(pathToFeatureCSV, 'r') as f:\n",
    "        csvReader = csv.reader(f, delimiter=',')\n",
    "        for row in csvReader:\n",
    "            data.append(np.array(row[1+numComma:]).astype(np.float))\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Featureset from saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureSet(pathToDirWithCSV, ourClasses, numSamplesPerClass = 855):\n",
    "    wholeDataSet = np.zeros([numSamplesPerClass*len(ourClasses), 68]);\n",
    "    Labels = np.zeros([numSamplesPerClass*len(ourClasses), 1])\n",
    "    \n",
    "    for i in range(0,len(ourClasses)):\n",
    "        c = ourClasses[i]\n",
    "        data = getFeatureMatrices(pathToDirWithCSV + c +\".csv\")\n",
    "        wholeDataSet[i*numSamplesPerClass : (i+1)*numSamplesPerClass, :] = data[:numSamplesPerClass,:]\n",
    "        Labels[i*numSamplesPerClass : (i+1)*numSamplesPerClass, :] = i*np.ones([numSamplesPerClass, 1])\n",
    "        \n",
    "    Labels = np.ravel(Labels)\n",
    "    \n",
    "    return wholeDataSet, Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Training and Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getError(clf, ourClasses, pathToDirWithTrainCSV, pathToDirWithTestCSV):\n",
    "    numCorrect = 0;\n",
    "    totalNum = 0;\n",
    "    for i in range(0,len(ourClasses)):\n",
    "        c = ourClasses[i]\n",
    "        data = getFeatureMatrices(pathToDirWithTrainCSV + c +\".csv\")\n",
    "        test = clf.predict(data);\n",
    "        numCorrect = numCorrect + np.sum(test == i)\n",
    "        totalNum   = totalNum + test.size\n",
    "    trainE = 1 - float(numCorrect)/float(totalNum)\n",
    "\n",
    "\n",
    "    for i in range(0,len(ourClasses)):\n",
    "        c = ourClasses[i]\n",
    "        data = getFeatureMatrices(pathToDirWithTestCSV + c +\".csv\")\n",
    "        test = clf.predict(data);\n",
    "        numCorrect = numCorrect + np.sum(test == i)\n",
    "        totalNum   = totalNum + test.size\n",
    "\n",
    "    testE = 1 - float(numCorrect)/float(totalNum)\n",
    "\n",
    "    return trainE, testE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wholeDataSet = preprocessing.normalize(wholeDataSet, norm='l2')\n",
    "\n",
    "print(\"Using LDA\")\n",
    "wholeDataSet, Labels = getFeatureSet(\"data/train_Features/\", ourClasses, numSamplesPerClass = 855)\n",
    "\n",
    "clf = LDA()\n",
    "clf.fit(wholeDataSet, Labels);\n",
    "LDA(n_components=None, priors=None, shrinkage=None, solver='svd',store_covariance=False, tol=0.1)\n",
    "\n",
    "trainE, testE = getError(clf, ourClasses, \"data/train_Features/\", \"data/test_Features/\")\n",
    "\n",
    "print(\"Train Error : {}\".format(trainE))\n",
    "print(\"Test Error  : {}\".format(testE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using Neural Network\")\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(50, 25, 10), random_state=1)\n",
    "clf.fit(wholeDataSet, Labels);\n",
    "\n",
    "trainE, testE = getError(clf, ourClasses, \"data/train_Features/\", \"data/test_Features/\")\n",
    "\n",
    "print(\"Train Error : {}\".format(trainE))\n",
    "print(\"Test Error  : {}\".format(testE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
